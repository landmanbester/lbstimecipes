_include:
  - kubeconfig.yaml

opts:
  backend:
    select: kube
    kube:
      enable: true
      dir: /mnt/data/pfb-test
      namespace: rarg-test-compute
      volumes:
        rarg-test-compute-efs-pvc:
          mount: /mnt/data

      user:
        uid: 1000
        gid: 1000

      predefined_pod_specs:
        admin:
          nodeSelector:
            rarg/node-class: admin
        scheduler:
          nodeSelector:
            rarg/node-class: compute
            rarg/instance-type: m6i.large
            rarg/capacity-type: ON_DEMAND
        tiny:
          nodeSelector:
            rarg/node-class: compute
            rarg/instance-type: m5.large
            rarg/capacity-type: ON_DEMAND
        thin:
          nodeSelector:
            rarg/node-class: compute
            rarg/instance-type: c6in.2xlarge
            rarg/capacity-type: ON_DEMAND
        pudgy:
          nodeSelector:
            rarg/node-class: compute
            rarg/instance-type: c6in.8xlarge
            rarg/capacity-type: ON_DEMAND
        fat:
          nodeSelector:
            rarg/node-class: compute
            rarg/instance-type: c6in.24xlarge
            rarg/capacity-type: ON_DEMAND

      job_pod:
        memory:
          limit: 0
        cpu:
          request:
            0
      dask_cluster:
        num_workers: 1
        scheduler_pod:
          type: scheduler
          memory:
            limit: "3Gi"
          cpu:
            request:
              1
        worker_pod:
          memory:
            limit: 0
          cpu:
            request:
              0

      env:
        NUMBA_CACHE_DIR: /mnt/data/pfb-test
        CONFIGURATT_CACHE_DIR: /mnt/data/pfb-test

image:
  steps:
    init:
      backend:
        select: singularity
        singularity:
          rebuild: true
          bind_dirs:
            /home/bester/numba_cache: rw
          env:
            NUMBA_CACHE_DIR: /home/bester/numba_cache

    grid:
      assign:
        # required for workers to get the correct env vars
        config.opts.backend.kube.env.NUMBA_NUM_THREADS: '30'
        config.opts.backend.kube.env.NUMEXPR_NUM_THREADS: '30'
        config.opts.backend.kube.env.JAX_ENABLE_X64: 'True'
        config.opts.backend.kube.env.JAX_PLATFORMS: 'cpu'
        config.opts.backend.kube.env.LD_LIBRARY_PATH: '/usr/local/lib'
      backend:
        select: kube
        kube:
          enable: true
          job_pod:  # This is where the main application runs.
            type: pudgy
            memory:
              limit: "27Gi"
            cpu:
              request: 28  # > than half available
          dask_cluster:  # Set up the Dask cluster.
            enable: true
            num_workers: 2
            name: pfb-test-cluster
            threads_per_worker: 1
            worker_pod:
              type: pudgy
              memory:
                limit: "27Gi"
              cpu:
                request: 28
          always_pull_images: true
      params:
        fits-mfs: false
        fits-cubes: false
        nworkers: 2
        nthreads: 30


    sara:
      assign:
        # we need this so the workers get the right env vars
        config.opts.backend.kube.env.NUMBA_NUM_THREADS: '90'
        config.opts.backend.kube.env.NUMEXPR_NUM_THREADS: '90'
        config.opts.backend.kube.env.JAX_ENABLE_X64: 'True'
        config.opts.backend.kube.env.JAX_PLATFORMS: 'cpu'
        config.opts.backend.kube.env.LD_LIBRARY_PATH: '/usr/local/lib'
      backend:
        select: kube
        kube:
          enable: true
          job_pod:  # This is where the main application runs.
            type: fat
            memory:
              limit: "170Gi"
            cpu:
              request: 50
          dask_cluster:  # no Dask cluster.
            enable: false
          always_pull_images: true
      params:
        fits-mfs: false
        fits-cubes: false
        nthreads: 90

    pull_model:
      cab: s3tolocal
      params:
        source: s3://ratt-public-data/ESO137/stage1_HI_I.mds
        dest: /home/bester/projects/ESO137/output/aws/
      tags: [compare]

    compare_models:
      cab: compmodels
      params:
        model-local: /scratch/bester/hi_combined_bda_I_main_model.mds
        model-remote: =previous.dest

cabs:
  s3tolocal:
    name: s3tolocal
    command: aws s3 cp
    policies:
      positional: true
    inputs:
      source:
        info: The location to fetch the model from
        dtype: URI
        required: true

    outputs:
      dest:
        info: The location to place the model
        required: true
        dtype: Directory
        mkdir: true

  compmodels:
    name: compmodels
    flavour: python-code
    backend:
      select: native
      native:
        virtualenv: ~/.venv/pfb
    command: |
      import numpy as np
      import xarray as xr
      import sympy as sm
      from pfb.utils.misc import model_from_mds

      mds1 = xr.open_zarr(model_local, chunks=None)
      mds2 = xr.open_zarr(model_remote, chunks=None)

      model1 = model_from_mds(mds1)
      model2 = model_from_mds(mds2)
      diff = model1 - model2
      try:
        assert np.allclose(1 + diff, atol=epsilon, rtol=epsilon)
      except:
        raise ValueError("Models don't match")

    inputs:
      model-local:
        info: Model obtained by executing locally
        required: true
        dtype: str
      model-remote:
        info: Model obtained by executing remotely
        required: true
        dtype: str
      epsilon:
        info: Precision with which to compare models
        default: 1e-4
        dtype: float
    outputs:
      {}
