opts:
  backend:
    kube:
      enable: false  # we don't use kube for all steps
      dir: /mnt/data/pfb-test
      namespace: rarg-test-compute
      volumes:
        rarg-test-compute-efs-pvc:
          mount: /mnt/data

      ## override user/group ID
      user:
        uid: 1000
        gid: 1000

      ## some predefined pod specs
      predefined_pod_specs:
        admin:
          nodeSelector:
            rarg/node-class: admin
        scheduler:
          nodeSelector:
            rarg/node-class: compute
            rarg/instance-type: m6i.large
            rarg/capacity-type: ON_DEMAND
        thin:
          nodeSelector:
            rarg/node-class: compute
            rarg/instance-type: c6in.2xlarge
            rarg/capacity-type: ON_DEMAND
        pudgy:
          nodeSelector:
            rarg/node-class: compute
            rarg/instance-type: c6in.8xlarge
            rarg/capacity-type: ON_DEMAND
        fat:
          nodeSelector:
            rarg/node-class: compute
            rarg/instance-type: c6in.24xlarge
            rarg/capacity-type: ON_DEMAND

      env:
        NUMBA_CACHE_DIR: /mnt/data/stimela-test
        CONFIGURATT_CACHE_DIR: /mnt/data/stimela-test

# cabs:
#   s3tolocal:
#     command: aws s3
#       inputs:

esoimage:
  steps:
    init:
      backend:
        native:  # need to configure to use singularity image
          enable: true

    grid:
      assign:
        # we need this so the workers get the right env vars
        config.opts.backend.kube.env.NUMBA_NUM_THREADS: '30'
        config.opts.backend.kube.env.JAX_ENABLE_X64: 'True'
        config.opts.backend.kube.env.LD_LIBRARY_PATH: '/usr/local/lib'
      backend:
        kube:
          enable: true
          job_pod:  # This is where the main application runs.
            type: pudgy
            memory_limit: "60Gi"
            cpu:
              request: 24  # > than half available
          dask_cluster:  # Set up the Dask cluster.
            enable: true
            num_workers: 10
            name: pfb-test-cluster
            threads_per_worker: 1
            worker_pod:
              type: pudgy
              memory_limit: "60Gi"
              cpu:
                request: 24
          always_pull_images: true

    sara:
      backend:
        kube:
          enable: true
          job_pod:  # This is where the main application runs.
            type: fat
            memory_limit: "185Gi"
            cpu:
              request: 90
          dask_cluster:  # no Dask cluster.
            enable: false
          always_pull_images: true

    # pull_model:
    #   cab: s3tolocal
    #   params:
    #     source: s3://path
    #     dest: /local/path
