_include:
  - kubeconfig.yaml
  - (cultcargo)quartical.yml
  - (cultcargo)pfb-imaging.yml

opts:  # This section is largely just to ensure that all these fields exist.
  backend:
    select: kube
    kube:
      context: 'lbester-rarg-test-eks-cluster'
      job_pod:
        memory:
          limit: 0
        cpu:
          request:
            0
      dask_cluster:
        num_workers: 1
        worker_pod:
          memory:
            limit: 0
          cpu:
            request:
              0

1gc:
  name: CROSSCAL
  info: Demo crosscal on AWS

  assign:
    ms1: s3://ratt-public-data/ESO137/ms1_target.zarr
    #ms2: s3://ratt-public-data/ESO137/ms2_target.zarr
    gain1: s3://ratt-public-data/ESO137/ms1_gains.qc/GKB-net
    #gain2: s3://ratt-public-data/ESO137/ms2_gains.qc/GKB-net

  assign_based_on:
    band:
      FULL:
        frange: 882541016:1626316406.25
      HI:
        frange: 1295000000:1503000000
      LO:
        frange: 980000000:1080000000

  inputs:
    band:
      default: HI
      info: FULL, HI or LO part of the band.
    basedir:
      default: 's3://rarg-test-binface/ESO137'
    log-directory:
      default: 'output/logs'
      aliases: ['*.log-directory']
    data-column:
      default: DATA
    sigma-column:
      default: SIGMA_SPECTRUM
      info: Original std of noise used to construct weights
    model-column:
      dtype: str
      default: MODEL_DATA
    nband:
      dtype: int
      default: 14
      info: Number of imaging bands
    scheduler:
      dtype: str
      default: distributed
      aliases: ['*.scheduler']
      info: Dask scheduler to use
    nthreads:
      dtype: int
      default: 64
    nworkers:
      dtype: int
      default: 32
    reset-cache:
      dtype: bool
      default: false

  steps:
    init:  # 1090.5248334407806s with 32 c6in.2xlarge with 1 dask-thread and 1 vthread
      cab: pfb.imit
      assign:
        config.opts.backend.kube.dask_cluster.num_workers: =recipe.nworkers
        config.opts.backend.kube.predefined_pod_specs.pudgy.nodeSelector.rarg/instance-type: c6in.2xlarge
        config.opts.backend.kube.job_pod.memory.limit: "14Gi"
        config.opts.backend.kube.job_pod.cpu.request: "6"
        config.opts.backend.kube.dask_cluster.worker_pod.memory.limit: "14Gi"
        config.opts.backend.kube.dask_cluster.worker_pod.cpu.request: "6"
      info: 'Initialise imaging data products'
      backend:
        kube:
          job_pod:  # This is where the main application runs.
            type: pudgy
          dask_cluster:  # Set up the dask cluster.
            enable: true
            name: pfb-test-cluster
            memory_limit: 0
            threads_per_worker: 1
            worker_pod:
              type: pudgy
          always_pull_images: true
      params:
        ms: [=recipe.ms1]
        output-filename: '{recipe.basedir}/stage1_{recipe.band}'
        data-column: =recipe.data-column
        sigma-column: =recipe.sigma-column
        gain-table: [=recipe.gain1]
        integrations-per-image: 150
        channels-per-image: 256
        nthreads-dask: 1
        nvthreads: 6
        nworkers: =recipe.nworkers
        scheduler: distributed
        chan-average: 4
        freq-range: =recipe.frange
        overwrite: true
        bda-decorr: 0.98

    deconv:
      cab: pfb.spotless
      assign:  # we need this so the workers get the right env vars
        config.opts.backend.kube.env.OMP_NUM_THREADS: '14'
        config.opts.backend.kube.env.OPENBLAS_NUM_THREADS: '14'
        config.opts.backend.kube.env.MKL_NUM_THREADS: '14'
        config.opts.backend.kube.env.VECLIB_MAXIMUM_THREADS: '14'
        config.opts.backend.kube.env.NUMBA_NUM_THREADS: '14'
        config.opts.backend.kube.env.NUMEXPR_NUM_THREADS: '14'
        config.opts.backend.kube.env.JAX_ENABLE_X64: 'True'
        config.opts.backend.kube.env.LD_LIBRARY_PATH: '/usr/local/lib'
      info: 'Initialise imaging data products'
      backend:
        kube:
          # debug:
          #   pause_on_start: true
          job_pod:  # This is where the main application runs.
            type: fat
            memory:
              limit: "115Gi"
            cpu:
              request: "14"
          dask_cluster:  # Set up the dask cluster.
            # scheduler_pod:
            #   type: scheduler
            #   memory:
            #     limit: "4Gi"
            #   cpu:
            #     request: "1"
            enable: true
            num_workers: 4
            name: pfb-test-cluster
            memory_limit: 0  # dask cluster setup
            threads_per_worker: 1
            worker_pod:  # resource specification for autoscaler
              type: fat
              memory:
                limit: "115Gi"
              cpu:
                request: "14"
          always_pull_images: true
      params:
        output-filename: '{recipe.basedir}/stage1_{recipe.band}'
        fits-cubes: false
        niter: 1
        bases: 'self,db1,db2,db3'
        nlevels: 3
        l1reweight-from: 6
        pd-tol: 3e-4
        pd-maxit: 50
        pd-verbose: 2
        pd-report-freq: 10
        tol: 1e-3
        rmsfactor: 1.25
        nthreads-dask: 1
        nvthreads: 14
        nworkers: 4
        nband: 4
        scheduler: distributed
        field-of-view: 2.25
        super-resolution-factor: 2.0
        robustness: -1.5
        mf-weighting: false
        l2-reweight-dof: 2.5
        reset-cache: =recipe.reset-cache
